{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "import sys\n",
    "from SwingyMonkey import SwingyMonkey\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.bcs.rochester.edu/people/robbie/jacobslab/cheat_sheet/ModelBasedRL.pdf\n",
    "\n",
    "http://mlg.eng.cam.ac.uk/mlss09/mlss_slides/Littman_1.pdf\n",
    "\n",
    "http://outlace.com/Reinforcement-Learning-Part-3/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main idea behind the model based system is to estimate and maximize : Rtotal(s, a)/N(s, a)\n",
    "Which is the average reward of an action (a) taken in state (s). In our specific case the state can be defined by the various parameter obtain from the game (tree, monkey):\n",
    "    \n",
    "    tree{dist, top, bot)\n",
    "    monkey{vel, top, bot}\n",
    "    \n",
    "One has to implement a sort of \"grid\" in order to go through all the potential state, taking each potential pixel would be tedious, it then make sense to define hyperparameters to define the grid size.\n",
    "\n",
    "    x_grid_px\n",
    "    y_grid_px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Main Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_grid_px = 20\n",
    "y_grid_px = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ModelBasedLearner:\n",
    "\n",
    "    def __init__(self, gamma):\n",
    "        self.previous_state  = None\n",
    "        self.previous_action = None\n",
    "        self.previous_reward = None\n",
    "        self.screen_height = 400\n",
    "        self.screen_width = 600\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        \n",
    "        # Get total number of states given the grid \n",
    "        self.total_states = ((self.screen_width/x_grid_px)*2 + 1) * ((self.screen_height/y_grid_px)*2 + 1)\n",
    "        \n",
    "        # Keep track of all states while running\n",
    "        self.list_of_states = [] #To store list of all States\n",
    "        \n",
    "        ### New Initialisation (Following Mark)\n",
    "        self.Q                   = defaultdict(lambda: [0,0]) #Q[States][Action]\n",
    "        self.R                   = defaultdict(lambda: [[0,1] , [0,1]]) # R[States][action][n,tot]\n",
    "        self.P                   = defaultdict(lambda: [[0,1] , [0,1]]) # R[States_prev, states][action][n,tot]\n",
    "        \n",
    "\n",
    "    def convert_state(self, state):\n",
    "        binsize = 50\n",
    "        dist_thresh    = 50\n",
    "        speed_thresh   = 0\n",
    "        \n",
    "        monkey_tree_top_dist = (state['monkey']['top'] - state['tree']['top']) / binsize   \n",
    "        #monkey_tree_bot_dist = (state['monkey']['bot'] - state['tree']['bot'])/ binsize\n",
    "\n",
    "        monkey_near_top      = (self.screen_height - state['monkey']['top']) < dist_thresh\n",
    "        monkey_near_bottom   = state['monkey']['bot']                 < dist_thresh\n",
    "        monkey_near_tree     = state['tree']['dist']                  < dist_thresh\n",
    "        monkey_fast          = state['monkey']['vel']                 > speed_thresh\n",
    "        return (monkey_near_top, monkey_near_bottom, monkey_near_tree, monkey_fast, monkey_tree_top_dist)\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        self.previous_state  = None\n",
    "        self.previous_action = None\n",
    "        self.previous_reward = None\n",
    "\n",
    "    # computes optimal policy and value function value at a given state\n",
    "    def get_best_value(self, state):   \n",
    "        if self.Q[state][1] > self.Q[state][0]:\n",
    "            return (self.Q[state][1],1)\n",
    "        else:\n",
    "            return (self.Q[state][0],0)\n",
    "    \n",
    "    def UpdateT(self, state_i):\n",
    "        self.P[self.previous_state, state_i][self.previous_action][0] += 1\n",
    "        # Update Count\n",
    "        for state in self.list_of_states:\n",
    "            self.P[self.previous_state, state][self.previous_action][1] += 1\n",
    "                \n",
    "    def UpdateR(self):\n",
    "        self.R[self.previous_state][self.previous_action][0] += self.previous_reward\n",
    "        # Update Count\n",
    "        self.R[self.previous_state][self.previous_action][1] += 1\n",
    "        \n",
    "    def GetProbability(self, previous_state, state, action):\n",
    "        # Prob = N / ToT\n",
    "        return self.P[previous_state,state][action][0] / self.P[previous_state,state][action][1]\n",
    "    \n",
    "    def GetAverageR(self, state, action):\n",
    "        return self.R[state][action][0] / self.R[state][action][1]\n",
    "        \n",
    "    def UpdateQ(self, state_i):\n",
    "        \n",
    "        # Update Q function (action = 0)\n",
    "        prob_x_Expectation = 0.0\n",
    "        for state in self.list_of_states:\n",
    "            prob_x_Expectation += (self.GetProbability(state_i, state, 0) * self.get_best_value(state)[0])\n",
    "        self.Q[state_i][0] = self.GetAverageR(state_i, 0) + (self.gamma * prob_x_Expectation)\n",
    "\n",
    "        # Update Q function (action = 1)\n",
    "        prob_x_Expectation = 0.0\n",
    "        for state in self.list_of_states:\n",
    "            prob_x_Expectation += (self.GetProbability(state_i,state,1) * self.get_best_value(state)[0])\n",
    "        self.Q[state_i][1] = self.GetAverageR(state_i, 1) + (self.gamma * prob_x_Expectation)\n",
    "             \n",
    "    \n",
    "    def action_callback(self, state):\n",
    "    \n",
    "        state_i = self.convert_state(state)\n",
    "\n",
    "        if self.previous_state != None:\n",
    "            self.UpdateT(state_i)\n",
    "            self.UpdateR()\n",
    "            self.UpdateQ(state_i)\n",
    "\n",
    "            # Decide which action\n",
    "            if self.Q[state_i][0] < self.Q[state_i][1]:\n",
    "                self.previous_action = 1\n",
    "            else:\n",
    "                self.previous_action = 0\n",
    "                \n",
    "        else:\n",
    "            self.previous_action = random.random() < 0.1\n",
    "\n",
    "        #Memory Save Previous state\n",
    "        self.previous_state = state_i\n",
    "\n",
    "        return self.previous_action\n",
    "\n",
    "    def reward_callback(self, reward):\n",
    "        self.previous_reward = reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_games(learner, hist, iters = 100, t_len = 1):\n",
    "    for ii in range(iters):\n",
    "        # Make a new monkey object.\n",
    "        swing = SwingyMonkey(sound=False,                  # Don't play sounds.\n",
    "                             text=\"Epoch %d\" % (ii),       # Display the epoch on screen.\n",
    "                             tick_length = t_len,          # Make game ticks super fast.\n",
    "                             action_callback=learner.action_callback,\n",
    "                             reward_callback=learner.reward_callback)\n",
    "\n",
    "        # Loop until you hit something.\n",
    "        while swing.game_loop():\n",
    "            pass\n",
    "        \n",
    "        # Save score history.\n",
    "        hist.append(swing.score)\n",
    "\n",
    "        # Reset the state of the learner.\n",
    "        learner.reset()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run(gamma):\n",
    "    agent = ModelBasedLearner(gamma)\n",
    "    scores = []\n",
    "    run_games(agent, scores) \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_scores = dict()\n",
    "gammas = [0.25, 0.5, 0.75, 1]\n",
    "for gamma in gammas:\n",
    "    scores = run(gamma)\n",
    "    all_scores[gamma] = scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "score_df = pd.DataFrame(columns=['gamma','total','max_score'])\n",
    "for i,(k,v) in enumerate(all_scores.iteritems()):\n",
    "    gamma = k\n",
    "    total =  sum(v)\n",
    "    max_score = max(v)\n",
    "    score_df.loc[i] = (gamma, total, max_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gamma</th>\n",
       "      <th>total</th>\n",
       "      <th>max_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.25</td>\n",
       "      <td>144.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.50</td>\n",
       "      <td>151.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.75</td>\n",
       "      <td>171.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.00</td>\n",
       "      <td>139.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gamma  total  max_score\n",
       "0   0.25  144.0       12.0\n",
       "1   0.50  151.0       16.0\n",
       "2   0.75  171.0       14.0\n",
       "3   1.00  139.0       14.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
